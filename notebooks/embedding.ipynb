{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding workflow using DINOv2\n",
    "\n",
    "This notebook focuses on the **Feature Extraction** pipeline. \n",
    "\n",
    "We utilize the fine-tuned model **ViTD2PC24All** ([DINOv2](https://dinov2.metademolab.com/)) to extract high-dimensional embeddings from the single-label train images and multi-label test images.\n",
    "\n",
    "We'll **visualize**, **tile**, and **process** these embeddings to support patch-wise multi-label inference using PyTorch and Faiss.\n",
    "\n",
    "![diagram](../images/pytorch-webinar-diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip list | grep pydantic\n",
    "# !uv pip install meerkat-ml\n",
    "# !which pip\n",
    "\n",
    "# !uv pip install pyspark -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to load the parquet file from disk and visualize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.precision = 2\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "root_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025\"\n",
    "dataset_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images\"\n",
    "hf_dataset_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/parquet/plantclef2025/full_test/HF_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_sdk/helpers.py:48: UserWarning: A newer version of lightning-sdk is available (0.2.14). Please consider upgrading with `pip install -U lightning-sdk`. Not all platform functionality can be guaranteed to work with the current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from plantclef.pytorch.data import HFPlantDataset\n",
    "from torchvision import transforms\n",
    "from typing import Callable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_dict(transforms: Callable, key: str) -> Callable:\n",
    "#     \"\"\"Apply transformation to a specific key in the dataset.\"\"\"\n",
    "\n",
    "#     def transform_fn(row):\n",
    "#         row[key] = [transforms(image) for image in row[key]]\n",
    "#         return row\n",
    "\n",
    "#     return transform_fn\n",
    "\n",
    "\n",
    "# def create_transform(image_size: int, key: Optional[str] = None) -> Callable:\n",
    "#     \"\"\"Create image transformation pipeline that maintains aspect ratio.\"\"\"\n",
    "#     transform_list = [\n",
    "#         # transforms.ToPILImage(),\n",
    "#         transforms.Resize(\n",
    "#             image_size, max_size=image_size + 2\n",
    "#         ),  # Maintains aspect ratio\n",
    "#         transforms.CenterCrop(image_size),\n",
    "#         transforms.ToTensor(),\n",
    "#     ]\n",
    "#     transform_list = transforms.Compose(transform_list)\n",
    "#     if key is not None:\n",
    "#         return transform_dict(transform_list, key)\n",
    "#     return transform_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running torch_pipeline with HFPlantDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.embed.workflow import torch_pipeline\n",
    "import os\n",
    "\n",
    "\n",
    "class Config:\n",
    "    use_grid: bool = True\n",
    "    grid_size: int = 3\n",
    "    image_size: int = 546\n",
    "    batch_size: int = 4\n",
    "    cpu_count: int = os.cpu_count() or 1\n",
    "    top_k: int = 5\n",
    "\n",
    "    root_dir: str = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025\"\n",
    "    dataset_dir: str = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images\"\n",
    "    hf_dataset_dir: str = \"/teamspace/studios/this_studio/plantclef-vision/data/parquet/plantclef2025/full_test/HF_dataset\"\n",
    "\n",
    "    embeddings_dir: str = None\n",
    "    test_embeddings_dir: str = None\n",
    "    folder_name: str = None\n",
    "    test_embeddings_path: str = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embeddings_dir = f\"{self.root_dir}/embeddings\"\n",
    "        self.test_embeddings_dir = f\"{self.embeddings_dir}/full_test\"\n",
    "        self.folder_name = f\"test_grid_{self.grid_size}x{self.grid_size}_embeddings\"\n",
    "        self.test_embeddings_path = f\"{self.test_embeddings_dir}/{self.folder_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = Config()\n",
    "\n",
    "\n",
    "# def make_predictions_and_save(\n",
    "#     cfg: Config,\n",
    "# ):\n",
    "#     \"\"\"Make predictions and save them to disk.\"\"\"\n",
    "#     # Create the directory if it doesn't exist\n",
    "#     os.makedirs(cfg.test_embeddings_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "#     ds = HFPlantDataset(\n",
    "#         path=cfg.hf_dataset_dir,\n",
    "#         transform=None,  # model.transform,\n",
    "#         col_name=\"image\",\n",
    "#         use_grid=cfg.use_grid,\n",
    "#         grid_size=cfg.grid_size,\n",
    "#     )\n",
    "\n",
    "#     ds.transform = ds.get_transforms(cfg.image_size)\n",
    "\n",
    "\n",
    "#     embeddings, logits = torch_pipeline(\n",
    "#         ds,\n",
    "#         batch_size=cfg.batch_size,\n",
    "#         use_grid=cfg.use_grid,\n",
    "#         grid_size=cfg.grid_size,\n",
    "#         cpu_count=cfg.cpu_count,\n",
    "#         top_k=cfg.top_k\n",
    "#     )\n",
    "\n",
    "#     pred_df = create_predictions_df(\n",
    "#         ds,\n",
    "#         embeddings,\n",
    "#         logits\n",
    "#     )\n",
    "\n",
    "#     pred_ds = HFDataset.from_pandas(pred_df)\n",
    "#     pred_ds.save_to_disk(test_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=518, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(518, 518))\n",
       "    MaybeToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plantclef.pytorch.model import DINOv2LightningModel\n",
    "\n",
    "top_k = 5\n",
    "model = DINOv2LightningModel(top_k=top_k)\n",
    "model.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 9, 3, 182, 182])\n"
     ]
    }
   ],
   "source": [
    "# for batch in dataloader:\n",
    "\n",
    "#     break\n",
    "\n",
    "# print(type(batch))\n",
    "\n",
    "# print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings and logits from model.predict_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9, 768])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# grid_size = cfg.grid_size\n",
    "\n",
    "# embeddings, logits = model.predict_step(\n",
    "#             batch, batch_idx=0\n",
    "#         )\n",
    "\n",
    "# embeddings, logits = model.predict_grid_step(\n",
    "#             batch, batch_idx=0\n",
    "#         )\n",
    "\n",
    "# print(embeddings.shape)\n",
    "# print(len(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9, 768])\n",
      "torch.Size([4, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "# print(embeddings.shape)\n",
    "# embeddings = embeddings.view(-1, grid_size**2, 768)\n",
    "# print(embeddings.shape)\n",
    "# embeddings = embeddings.view(-1, grid_size**2, 768)\n",
    "\n",
    "# logits = [\n",
    "#             logits[i : i + grid_size**2] for i in range(0, len(logits), grid_size**2)\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9, 768])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# print(embeddings.shape)\n",
    "# print(len(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 768])\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# print(embeddings.shape)\n",
    "# print(len(logits))\n",
    "# print([l.keys() for l in logits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1395063': 0.32624733448028564,\n",
       " '1395117': 0.06265220791101456,\n",
       " '1664563': 0.02622845023870468,\n",
       " '1394850': 0.02485588751733303,\n",
       " '1360224': 0.024763811379671097}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size -- len(logits): 36\n",
      "grid_size**2 -- len(logits[0]): 5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_size**2 -- len(logits[0]): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(logits[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# logits_img0_tile0 = logits[0][0]\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k -- k = len(list(logits[0][0].keys())): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# print(f\"batch_size -- len(logits): {len(logits)}\")\n",
    "# print(f\"grid_size**2 -- len(logits[0]): {len(logits[0])}\")\n",
    "# # logits_img0_tile0 = logits[0][0]\n",
    "# print(f\"top_k -- k = len(list(logits[0][0].keys())): {len(list(logits[0][0].keys()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image names from HFDataset -> Create a pandas DataFrame to match image names to logits + embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings, logits = model.predict_grid_step(\n",
    "#             batch, batch_idx=0\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_predictions_df(\n",
    "#     ds: HFPlantDataset, embeddings: torch.Tensor, logits: list\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Accepts an HFPlantDataset and a set of embeddings and logits.\n",
    "\n",
    "#     To be called after the model has been run on the full dataset in ds.\n",
    "\n",
    "#     Returns a DataFrame with the following columns:\n",
    "#         - image_name\n",
    "#         - tile\n",
    "#         - embeddings\n",
    "#         - logits\n",
    "#     The DataFrame is exploded to have one row per tile.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     pred_df = pd.DataFrame({\"image_name\": ds.dataset[\"file_path\"]})\n",
    "#     pred_df[\"image_name\"] = pred_df[\"image_name\"].str.rsplit(\"/\", n=1, expand=True)[1]\n",
    "\n",
    "#     pred_df = pred_df.convert_dtypes()\n",
    "\n",
    "#     pred_df = pred_df.assign(embeddings=embeddings.cpu().tolist(), logits=logits)\n",
    "#     explode_df = pred_df.explode([\"embeddings\", \"logits\"], ignore_index=True)\n",
    "#     explode_df = explode_df.assign(tile=explode_df.groupby(\"image_name\").cumcount())\n",
    "\n",
    "#     return explode_df\n",
    "\n",
    "\n",
    "# pred_ds = HFDataset.from_pandas(explode_df)\n",
    "# pred_ds.save_to_disk(test_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_ds = Dataset.load_from_disk(test_embeddings_path)\n",
    "# loaded_ds.features[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   file_path   4 non-null      string\n",
      " 1   embeddings  4 non-null      object\n",
      " 2   logits      4 non-null      object\n",
      "dtypes: object(2), string(1)\n",
      "memory usage: 224.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def write_embeddings_to_parquet(\n",
    "    df: pd.DataFrame,\n",
    "    folder_name: str,\n",
    "    num_partitions: int = 20,\n",
    "):\n",
    "    # path to data\n",
    "    root = Path().resolve().parents[0]\n",
    "    data_path = f\"{root}/data/embeddings\"\n",
    "    output_path = f\"{data_path}/{folder_name}\"\n",
    "\n",
    "    # remove existing data if it exists to avoid duplication\n",
    "    if Path(output_path).exists():\n",
    "        shutil.rmtree(output_path, ignore_errors=True)\n",
    "\n",
    "    # convert logits to json strings\n",
    "    df[\"logits\"] = df[\"logits\"].apply(json.dumps)\n",
    "\n",
    "    # assign partition numbers (0 to num_partitions-1)\n",
    "    df_size = len(df)\n",
    "    df[\"partition\"] = np.repeat(\n",
    "        np.arange(num_partitions), np.ceil(df_size / num_partitions)\n",
    "    )[:df_size]\n",
    "\n",
    "    # write to parquet using the new partition column\n",
    "    df.to_parquet(output_path, partition_cols=[\"partition\"], index=False)\n",
    "\n",
    "    print(\n",
    "        f\"Embedding dataset written to: {output_path} with {num_partitions} partitions.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# write data\n",
    "# folder_name = f\"test_grid_{GRID_SIZE}x{GRID_SIZE}_embeddings\"\n",
    "# write_embeddings_to_parquet(test_explode_df, folder_name, num_partitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.plot_image_tiles(idx=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.dataset = ds.dataset.take(100)\n",
    "# # extract embeddings\n",
    "# embeddings, logits = torch_pipeline(\n",
    "#     dataset=ds,  # .dataset.take(5),\n",
    "#     batch_size=2,\n",
    "#     use_grid=True,\n",
    "#     cpu_count=1,\n",
    "# )\n",
    "# embeddings.shape\n",
    "# grid_size = 3\n",
    "\n",
    "# embeddings = embeddings.view(-1, grid_size**2, 768)\n",
    "# embeddings.shape\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# img = ds._get_image_tensor(0)\n",
    "\n",
    "# plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(image: torch.Tensor) -> torch.Tensor:\n",
    "    min_dim = min(image.shape[1:])\n",
    "    return transforms.CenterCrop(min_dim)(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save huggingface test set to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting file paths in /teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images: 100%|██████████| 2105/2105 [00:00<00:00, 837905.47it/s]\n",
      "Walking through dir /teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images: 1it [00:00, 20.22it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7550fa41ec4b4ebb40c9c2b7182dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/18 shards):   0%|          | 0/2105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image_list = collect_image_filepaths(dataset_dir)\n",
    "\n",
    "# ds = Dataset.from_dict({\"image\": image_list})\n",
    "# ds = ds.cast_column(\"image\", Image())\n",
    "\n",
    "# ds.save_to_disk(hf_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d29f25ba1748439d3c32c989d6d6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ds_loaded = Dataset.load_from_disk(hf_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transform(image_size: int) -> Callable:\n",
    "    \"\"\"Create image transformation pipeline that maintains aspect ratio.\"\"\"\n",
    "    transform_list = [\n",
    "        # transforms.ToPILImage(),\n",
    "        transforms.Resize(\n",
    "            image_size, max_size=image_size + 2\n",
    "        ),  # Maintains aspect ratio\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    "\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ds_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from plantclef.config import get_device\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Get list of stored filed in cloud bucket\n",
    "root = Path().resolve().parents[0]\n",
    "print(root)\n",
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "test_parquet_output_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/parquet/plantclef2025/full_test\"\n",
    "os.makedirs(test_parquet_output_dir, exist_ok=True)\n",
    "\n",
    "root = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025\"\n",
    "test_image_dir = (\n",
    "    root + \"/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting embeddings from single-label training images\n",
    "\n",
    "We extract embeddings from a small subset of training images to validate our pipeline.  \n",
    "We don't perform tiling on the train images (we use the full image) and extract 768-dimensional ViT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_train_df = pd.DataFrame({})\n",
    "\n",
    "# extract embeddings\n",
    "embeddings, logits = torch_pipeline(\n",
    "    limit_train_df,\n",
    "    batch_size=2,\n",
    "    use_grid=False,\n",
    "    cpu_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings shape\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first embedding\n",
    "embeddings[0][0][:100]  # showing first 100 values out of 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings dataframe\n",
    "cols = [\"image_name\", \"data\", \"species\", \"species_id\"]\n",
    "embeddings_df = limit_train_df[cols].copy()\n",
    "embeddings_df[\"embeddings\"] = embeddings.tolist()\n",
    "embeddings_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_images_from_binary\n",
    "\n",
    "embeddings_df = pd.DataFrame()\n",
    "plot_images_from_binary(\n",
    "    embeddings_df,\n",
    "    data_col=\"data\",\n",
    "    label_col=\"species\",\n",
    "    grid_size=(1, 2),\n",
    "    crop_square=True,\n",
    "    figsize=(8, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_embeddings\n",
    "\n",
    "plot_embeddings(\n",
    "    embeddings_df,\n",
    "    data_col=\"embeddings\",\n",
    "    label_col=\"species\",\n",
    "    grid_size=(1, 2),\n",
    "    figsize=(8, 4),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding test images with tiling (3x3)\n",
    "\n",
    "\n",
    "Since the test images are high-resolution and contain multiple plant species, we split them into a 3x3 grid of tiles.\n",
    "- We **extract embeddings** and **top-*K* logits** from each tile using the ViT model.  \n",
    "- This **patch-wise representation** is critical for enabling multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "USE_GRID = True\n",
    "GRID_SIZE = 3  # 3x3 grid of tiles\n",
    "CPU_COUNT = 1  # custom cpu_count\n",
    "TOP_K = 5  # top-K logits for each tile\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame({})\n",
    "test_image_df = pd.DataFrame({})\n",
    "\n",
    "# select images from test set\n",
    "image_names = [\"CBN-Pyr-03-20230706.jpg\", \"CBN-can-E6-20230706.jpg\"]\n",
    "test_image_df = test_df[test_df[\"image_name\"].isin(image_names)]\n",
    "\n",
    "# get embeddings and logits\n",
    "embeddings, logits = torch_pipeline(\n",
    "    test_image_df,\n",
    "    batch_size=2,\n",
    "    use_grid=USE_GRID,\n",
    "    grid_size=GRID_SIZE,\n",
    "    cpu_count=CPU_COUNT,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings shape\n",
    "embeddings.shape  # (2, 9, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings dataframe\n",
    "def explode_embeddings_logits(\n",
    "    df: pd.DataFrame,\n",
    "    embeddings: torch.Tensor,\n",
    "    logits: list,\n",
    "    cols: list = [\"image_name\", \"data\"],\n",
    ") -> pd.DataFrame:\n",
    "    # create dataframe\n",
    "    pred_df = df[cols].copy()\n",
    "    pred_df[\"embeddings\"] = embeddings.cpu().tolist()\n",
    "    pred_df[\"logits\"] = logits\n",
    "    # explode embeddings\n",
    "    explode_df = pred_df.explode([\"embeddings\", \"logits\"], ignore_index=True)\n",
    "    # assign tile number for each image\n",
    "    explode_df[\"tile\"] = explode_df.groupby(\"image_name\").cumcount()\n",
    "    return explode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_df = explode_embeddings_logits(test_image_df, embeddings, logits)\n",
    "explode_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_image_tiles\n",
    "\n",
    "# show image tiles\n",
    "plot_image_tiles(\n",
    "    explode_df,\n",
    "    data_col=\"data\",\n",
    "    grid_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_embed_tiles\n",
    "\n",
    "plot_embed_tiles(\n",
    "    explode_df,\n",
    "    data_col=\"embeddings\",\n",
    "    grid_size=3,\n",
    "    figsize=(15, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot grid embeddings\n",
    "plot_embeddings(\n",
    "    explode_df,\n",
    "    data_col=\"embeddings\",\n",
    "    label_col=\"tile\",\n",
    "    grid_size=(3, 3),\n",
    "    figsize=(8, 8),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing classifier logits per tile\n",
    "\n",
    "For each tile, we look at the **top predicted species** and associated confidence scores (`logits`).  \n",
    "This helps interpret how confident the model is in identifying species in each patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length logits: {len(logits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display logits of first tile\n",
    "explode_df[\"logits\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display logits for each tile\n",
    "for i in range(9):\n",
    "    logits = explode_df[\"logits\"].iloc[i]\n",
    "    logits_formatted = {k: round(v, 3) for k, v in logits.items()}\n",
    "    print(f\"Tile {i+1}: {logits_formatted}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the entire test set with tiling\n",
    "\n",
    "We scale up our embedding pipeline to process the full test dataset using **3x3 tiling**.  \n",
    "This prepares the data for the downstream tasks of efficient **nearest neighbor search** and **multi-label prediction** at the tile level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cpu_count = os.cpu_count()\n",
    "print(f\"CPU count: {cpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "USE_GRID = True\n",
    "GRID_SIZE = 3  # 3x3 grid of tiles\n",
    "CPU_COUNT = 1  # custom cpu_count\n",
    "TOP_K = 5  # top-K logits for each tile\n",
    "\n",
    "# get embeddings and logits\n",
    "test_embeddings, test_logits = torch_pipeline(\n",
    "    test_df,\n",
    "    batch_size=10,  # 10 imamges per batch\n",
    "    use_grid=USE_GRID,\n",
    "    grid_size=GRID_SIZE,\n",
    "    cpu_count=CPU_COUNT,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_embeddings.shape)\n",
    "print(len(test_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode full embeddings and logits\n",
    "test_explode_df = explode_embeddings_logits(\n",
    "    test_df,\n",
    "    test_embeddings,\n",
    "    test_logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_explode_df.shape)\n",
    "test_explode_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embed_tiles(\n",
    "    test_explode_df,\n",
    "    data_col=\"embeddings\",\n",
    "    grid_size=3,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving test embeddings and logits to Parquet\n",
    "\n",
    "We serialize the full test embeddings into partitioned Parquet files for later use in inference pipelines.  \n",
    "The logits are stored as JSON strings for flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_embeddings_to_parquet(\n",
    "#     df: pd.DataFrame,\n",
    "#     folder_name: str,\n",
    "#     num_partitions: int = 20,\n",
    "# ):\n",
    "#     # path to data\n",
    "#     root = Path().resolve().parents[0]\n",
    "#     data_path = f\"{root}/data/embeddings\"\n",
    "#     output_path = f\"{data_path}/{folder_name}\"\n",
    "\n",
    "#     # remove existing data if it exists to avoid duplication\n",
    "#     if Path(output_path).exists():\n",
    "#         shutil.rmtree(output_path, ignore_errors=True)\n",
    "\n",
    "#     # convert logits to json strings\n",
    "#     df[\"logits\"] = df[\"logits\"].apply(json.dumps)\n",
    "\n",
    "#     # assign partition numbers (0 to num_partitions-1)\n",
    "#     df_size = len(df)\n",
    "#     df[\"partition\"] = np.repeat(\n",
    "#         np.arange(num_partitions), np.ceil(df_size / num_partitions)\n",
    "#     )[:df_size]\n",
    "\n",
    "#     # write to parquet using the new partition column\n",
    "#     df.to_parquet(output_path, partition_cols=[\"partition\"], index=False)\n",
    "\n",
    "#     print(\n",
    "#         f\"Embedding dataset written to: {output_path} with {num_partitions} partitions.\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # write data\n",
    "# folder_name = f\"test_grid_{GRID_SIZE}x{GRID_SIZE}_embeddings\"\n",
    "# write_embeddings_to_parquet(test_explode_df, folder_name, num_partitions=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the full training set (no tiling)\n",
    "\n",
    "We repeat the embedding process on the **full training dataset**, this time *without tiling*.  \n",
    "This enables us to use the embeddings directly or as a **transfer learning** approach in a Faiss-based nearest neighbor retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "USE_GRID = False\n",
    "CPU_COUNT = 1  # custom cpu_count\n",
    "TOP_K = 5  # top-K logits for each tile\n",
    "\n",
    "train_df = pd.DataFrame({})\n",
    "\n",
    "# get embeddings and logits\n",
    "train_embeddings, train_logits = torch_pipeline(\n",
    "    train_df,\n",
    "    batch_size=20,  # 20 imamges per batch\n",
    "    use_grid=USE_GRID,\n",
    "    cpu_count=CPU_COUNT,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embeddings.shape)\n",
    "print(len(train_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode full embeddings and logits\n",
    "train_explode_df = explode_embeddings_logits(\n",
    "    train_df,\n",
    "    train_embeddings,\n",
    "    train_logits,\n",
    "    cols=[\"image_name\", \"data\", \"species\", \"species_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explode_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_single_image_embeddings\n",
    "\n",
    "plot_single_image_embeddings(\n",
    "    train_explode_df,\n",
    "    num_images=2,\n",
    "    figsize=(8, 10),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the training embeddings to Parquet\n",
    "\n",
    "Finally, we save the full training embeddings in partitioned Parquet format to support fast, distributed retrieval during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data\n",
    "folder_name = \"train_embeddings\"\n",
    "write_embeddings_to_parquet(train_explode_df, folder_name, num_partitions=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Ready for Downstream Use\n",
    "\n",
    "We now have rich ViT embeddings for both train and test datasets, ready for use in:\n",
    "- Multi-label classification\n",
    "- Retrieval-based inference\n",
    "- Nearest Neighbor Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "data_path = f\"{root}/data/embeddings\"\n",
    "# output_path = f\"{data_path}/test_grid_3x3_embeddings\"\n",
    "output_path = f\"{data_path}/train_embeddings\"\n",
    "\n",
    "train_emb_df = pd.read_parquet(output_path)\n",
    "print(train_emb_df.shape)\n",
    "train_emb_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{data_path}/test_grid_3x3_embeddings\"\n",
    "test_grid_df = pd.read_parquet(output_path)\n",
    "print(test_grid_df.shape)\n",
    "test_grid_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
