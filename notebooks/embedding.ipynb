{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding workflow using DINOv2\n",
    "\n",
    "This notebook focuses on the **Feature Extraction** pipeline. \n",
    "\n",
    "We utilize the fine-tuned model **ViTD2PC24All** ([DINOv2](https://dinov2.metademolab.com/)) to extract high-dimensional embeddings from the single-label train images and multi-label test images.\n",
    "\n",
    "We'll **visualize**, **tile**, and **process** these embeddings to support patch-wise multi-label inference using PyTorch and Faiss.\n",
    "\n",
    "![diagram](../images/pytorch-webinar-diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip list | grep pydantic\n",
    "# !uv pip install meerkat-ml\n",
    "# !which pip\n",
    "\n",
    "# !uv pip install pyspark -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to load the parquet file from disk and visualize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.precision = 2\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "root_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025\"\n",
    "dataset_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images\"\n",
    "hf_dataset_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/parquet/plantclef2025/full_test/HF_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plantclef.pytorch.data import HFPlantDataset\n",
    "from torchvision import transforms\n",
    "from typing import Callable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_dict(transforms: Callable, key: str) -> Callable:\n",
    "#     \"\"\"Apply transformation to a specific key in the dataset.\"\"\"\n",
    "\n",
    "#     def transform_fn(row):\n",
    "#         row[key] = [transforms(image) for image in row[key]]\n",
    "#         return row\n",
    "\n",
    "#     return transform_fn\n",
    "\n",
    "\n",
    "# def create_transform(image_size: int, key: Optional[str] = None) -> Callable:\n",
    "#     \"\"\"Create image transformation pipeline that maintains aspect ratio.\"\"\"\n",
    "#     transform_list = [\n",
    "#         # transforms.ToPILImage(),\n",
    "#         transforms.Resize(\n",
    "#             image_size, max_size=image_size + 2\n",
    "#         ),  # Maintains aspect ratio\n",
    "#         transforms.CenterCrop(image_size),\n",
    "#         transforms.ToTensor(),\n",
    "#     ]\n",
    "#     transform_list = transforms.Compose(transform_list)\n",
    "#     if key is not None:\n",
    "#         return transform_dict(transform_list, key)\n",
    "#     return transform_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running torch_pipeline with HFPlantDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_sdk/helpers.py:48: UserWarning: A newer version of lightning-sdk is available (0.2.14). Please consider upgrading with `pip install -U lightning-sdk`. Not all platform functionality can be guaranteed to work with the current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Config</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">use_grid</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">grid_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">image_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">546</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cpu_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">root_dir</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_dir</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">025_test_images/PlantCLEF2025_test_images'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">hf_dataset_dir</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/plantclef-vision/data/parquet/plantclef2025/full_test/HF_dataset</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">embeddings_dir</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">test_embeddings_dir</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings/full_test'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">folder_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_grid_3x3_embeddings'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">test_embeddings_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings/full_test/t</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">est_grid_3x3_embeddings'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33muse_grid\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mgrid_size\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "    \u001b[33mimage_size\u001b[0m=\u001b[1;36m546\u001b[0m,\n",
       "    \u001b[33mbatch_size\u001b[0m=\u001b[1;36m16\u001b[0m,\n",
       "    \u001b[33mcpu_count\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
       "    \u001b[33mtop_k\u001b[0m=\u001b[1;36m5\u001b[0m,\n",
       "    \u001b[33mdevice\u001b[0m=\u001b[32m'cpu'\u001b[0m,\n",
       "    \u001b[33mroot_dir\u001b[0m=\u001b[32m'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025'\u001b[0m,\n",
       "    \u001b[33mdataset_dir\u001b[0m=\u001b[32m'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2\u001b[0m\n",
       "\u001b[32m025_test_images/PlantCLEF2025_test_images'\u001b[0m,\n",
       "    \u001b[33mhf_dataset_dir\u001b[0m=\u001b[32m'/teamspace/studios/this_studio/plantclef-vision/data/parquet/plantclef2025/full_test/HF_dataset\u001b[0m\n",
       "\u001b[32m'\u001b[0m,\n",
       "    \u001b[33membeddings_dir\u001b[0m=\u001b[32m'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings'\u001b[0m,\n",
       "    \u001b[33mtest_embeddings_dir\u001b[0m=\u001b[32m'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings/full_test'\u001b[0m,\n",
       "    \u001b[33mfolder_name\u001b[0m=\u001b[32m'test_grid_3x3_embeddings'\u001b[0m,\n",
       "    \u001b[33mtest_embeddings_path\u001b[0m=\u001b[32m'/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings/full_test/t\u001b[0m\n",
       "\u001b[32mest_grid_3x3_embeddings'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.embed.workflow import torch_pipeline, Config\n",
    "from plantclef.embed.utils import print_dir_size\n",
    "import os\n",
    "from rich import print as pprint\n",
    "\n",
    "cfg = Config()\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing disk usage of directory: /teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings/full_test/test_grid_3x3_embeddings\n",
      "Directory Disk Usage: 543M\t/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/embeddings/full_test/test_grid_3x3_embeddings\n",
      "2025-05-08 08:42:53\n"
     ]
    }
   ],
   "source": [
    "print_dir_size(cfg.test_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(species_ids): 2911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_name', 'embeddings', 'logits', 'tile'],\n",
       "    num_rows: 18945\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset as HFDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = HFDataset.load_from_disk(cfg.test_embeddings_path)\n",
    "\n",
    "\n",
    "species_ids = [\n",
    "    int(species_id) for species_id in sorted(list(set(ds[0][\"logits\"].keys())))\n",
    "]\n",
    "print(f\"len(species_ids): {len(species_ids)}\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def remove_NaN_values_from_dict(d: dict) -> dict:\n",
    "    \"\"\"Remove NaN values from a dictionary.\"\"\"\n",
    "    return {k: v for k, v in d.items() if v is not None}\n",
    "\n",
    "\n",
    "def sort_and_filter_dict(d: dict, top_k: int = 0) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Sort a dictionary by values and filter out NaN values.\n",
    "\n",
    "    Takes in a dictionary mapping str keys to float values, then\n",
    "        removes any keys with NaN values\n",
    "        sorts the remaining key-value pairs in descending order by value and transforms into a sorted list of tuples.\n",
    "    If top_k is specified, only the top_k items are returned.\n",
    "    Args:\n",
    "        d (dict): The dictionary to sort and filter.\n",
    "        top_k (int): The number of top items to return. Default is 0, which returns all items.\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: A sorted list of tuples (key, value) from the dictionary.\n",
    "\n",
    "    * [TODO] -- Consider adding a threshold for the values to filter out low-confidence predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    d = remove_NaN_values_from_dict(d)\n",
    "\n",
    "    # Sort the dictionary by values in descending order\n",
    "    sorted_list = sorted(d.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # If top_k is specified, return only the top_k items\n",
    "    if top_k > 0:\n",
    "        sorted_list = sorted_list[:top_k]\n",
    "\n",
    "    return sorted_list\n",
    "\n",
    "\n",
    "# def format_logits(\n",
    "#     row: dict, key: Optional[str] = None\n",
    "# ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "#     \"\"\"\n",
    "#     Format the logits dictionary to remove NaN values and sort by confidence.\n",
    "\n",
    "#     Args:\n",
    "#         row (dict): The dictionary containing logits.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A formatted dictionary with sorted logits.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Sort the dictionary by values in descending order\n",
    "#     if isinstance(key, str):\n",
    "#         row = row[key]\n",
    "#     else:\n",
    "#         key = \"\"\n",
    "#     logits = sort_and_filter_dict(row, top_k=5)\n",
    "\n",
    "#     return {key: logits}\n",
    "\n",
    "\n",
    "# row = remove_NaN_values_from_dict(ds[0][\"logits\"])\n",
    "\n",
    "# row = ds[2100] #[\"logits\"]\n",
    "# format_logits(row)\n",
    "# sort_and_filter_dict(row, top_k=5)\n",
    "\n",
    "\n",
    "# sorted(row.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import flatten\n",
    "import more_itertools as mit\n",
    "\n",
    "\n",
    "def select_top_k_unique_logits(df: pd.DataFrame, top_k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Select the top k unique logits from the DataFrame.\n",
    "    \"\"\"\n",
    "    assert df.shape == (9, 2)\n",
    "\n",
    "    # img_name, g = next(iter(dfg))\n",
    "    logits = sorted(flatten(df[\"logits\"].to_list()), key=lambda x: x[1], reverse=True)\n",
    "    logits_unique = list(mit.unique_everseen(logits, key=lambda x: x[0]))\n",
    "\n",
    "    top_k_logits_unique = logits_unique[:top_k]\n",
    "\n",
    "    return top_k_logits_unique\n",
    "\n",
    "\n",
    "def groupby_image_select_top_k_unique_logits(\n",
    "    df: pd.DataFrame, top_k: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group by image across all image tiles and select the top k unique logits.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        top_k (int): The number of top items to include. Default is 5.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the selected logits.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = df.groupby(\"image_name\")\n",
    "\n",
    "    return (\n",
    "        df.apply(select_top_k_unique_logits, top_k=top_k).rename(\"logits\").reset_index()\n",
    "    )\n",
    "\n",
    "    # logits = sorted(flatten(df[\"logits\"].to_list()), key=lambda x: x[1], reverse=True)\n",
    "    # logits = list(mit.unique_everseen(logits, key=lambda x: x[0]))\n",
    "    # return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = pd.Series([0, 1, 2])\n",
    "ddf\n",
    "ddf.rename(\"col_name\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "\n",
    "def prepare_submission_csv(ds: HFDataset, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a submission CSV file from logits saved to disk as a Hugging Face dataset.\n",
    "\n",
    "    Args:\n",
    "        ds (HFDataset): The dataset to process.\n",
    "            Expected columns are  ['image_name', 'embeddings', 'logits', 'tile'].\n",
    "        top_k (int): The number of top items to include in the submission. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): A DataFrame containing the formatted top_k species_id predictions.\n",
    "            Expected columns are ['quadrat_id', 'species_ids'].\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert the [\"image_name\", \"logits\"] columns from the hf dataset in to a pd.DataFrame\n",
    "    df = ds.remove_columns([\"embeddings\", \"tile\"]).to_pandas()\n",
    "\n",
    "    df = df.assign(\n",
    "        logits=df.apply(\n",
    "            lambda x: sort_and_filter_dict(  # Sort species IDs from high-to-low confidence scores remove all but the top_k\n",
    "                x[\"logits\"], top_k=top_k\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = groupby_image_select_top_k_unique_logits(df, top_k=5)\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"image_name\": \"quadrat_id\",\n",
    "            \"logits\": \"species_ids\",\n",
    "        }\n",
    "    )\n",
    "    df = df.assign(\n",
    "        species_ids=df.apply(\n",
    "            lambda x: [  # Select only the species IDs\n",
    "                species_id for species_id, _ in x[\"species_ids\"]\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33614/1293469598.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.apply(select_top_k_unique_logits, top_k=top_k).rename(\"logits\").reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-CEV3-20240602.jpg</td>\n",
       "      <td>[(1654010, 0.44266772270202637), (1395063, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBN-PdlC-A1-20130807.jpg</td>\n",
       "      <td>[(1744569, 0.2301855832338333), (1361917, 0.22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBN-PdlC-A1-20130903.jpg</td>\n",
       "      <td>[(1744569, 0.16917195916175842), (1392608, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBN-PdlC-A1-20140721.jpg</td>\n",
       "      <td>[(1529289, 0.14910352230072021), (1374758, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBN-PdlC-A1-20140811.jpg</td>\n",
       "      <td>[(1361281, 0.12936192750930786), (1418612, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>RNNB-8-5-20240118.jpg</td>\n",
       "      <td>[(1361437, 0.7179210782051086), (1655199, 0.52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>RNNB-8-6-20240118.jpg</td>\n",
       "      <td>[(1655199, 0.37736761569976807), (1363434, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>RNNB-8-7-20240118.jpg</td>\n",
       "      <td>[(1359297, 0.30361855030059814), (1356521, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>RNNB-8-8-20240118.jpg</td>\n",
       "      <td>[(1359650, 0.3005388379096985), (1396330, 0.28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>RNNB-8-9-20240118.jpg</td>\n",
       "      <td>[(1655199, 0.21261119842529297), (1361437, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image_name  \\\n",
       "0       2024-CEV3-20240602.jpg   \n",
       "1     CBN-PdlC-A1-20130807.jpg   \n",
       "2     CBN-PdlC-A1-20130903.jpg   \n",
       "3     CBN-PdlC-A1-20140721.jpg   \n",
       "4     CBN-PdlC-A1-20140811.jpg   \n",
       "...                        ...   \n",
       "2100     RNNB-8-5-20240118.jpg   \n",
       "2101     RNNB-8-6-20240118.jpg   \n",
       "2102     RNNB-8-7-20240118.jpg   \n",
       "2103     RNNB-8-8-20240118.jpg   \n",
       "2104     RNNB-8-9-20240118.jpg   \n",
       "\n",
       "                                                 logits  \n",
       "0     [(1654010, 0.44266772270202637), (1395063, 0.3...  \n",
       "1     [(1744569, 0.2301855832338333), (1361917, 0.22...  \n",
       "2     [(1744569, 0.16917195916175842), (1392608, 0.1...  \n",
       "3     [(1529289, 0.14910352230072021), (1374758, 0.1...  \n",
       "4     [(1361281, 0.12936192750930786), (1418612, 0.1...  \n",
       "...                                                 ...  \n",
       "2100  [(1361437, 0.7179210782051086), (1655199, 0.52...  \n",
       "2101  [(1655199, 0.37736761569976807), (1363434, 0.2...  \n",
       "2102  [(1359297, 0.30361855030059814), (1356521, 0.2...  \n",
       "2103  [(1359650, 0.3005388379096985), (1396330, 0.28...  \n",
       "2104  [(1655199, 0.21261119842529297), (1361437, 0.2...  \n",
       "\n",
       "[2105 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "top_1: [('1654010', 0.44266772270202637), ('1744569', 0.2301855832338333), ('1744569', 0.16917195916175842), ('1529289', 0.14910352230072021), ('1361281', 0.12936192750930786), ('1744569', 0.1826046109199524)]\n",
      "top_2: [('1395063', 0.3262457549571991), ('1361917', 0.22664865851402283), ('1392608', 0.15455718338489532), ('1374758', 0.12966282665729523), ('1418612', 0.11788784712553024), ('1358492', 0.14449474215507507)]\n",
      "top_3: [('1392662', 0.3107529878616333), ('1356350', 0.22245322167873383), ('1361382', 0.11053255945444107), ('1402995', 0.08004958182573318), ('1356350', 0.08978088945150375), ('1361524', 0.12258566915988922)]\n",
      "top_4: [('1414387', 0.2375417947769165), ('1418612', 0.17090369760990143), ('1361068', 0.07495987415313721), ('1741880', 0.07896480709314346), ('1392608', 0.08292622119188309), ('1397565', 0.10479555279016495)]\n",
      "top_5: [('1743646', 0.17830075323581696), ('1361129', 0.1368836909532547), ('1361971', 0.06213787570595741), ('1362066', 0.072990283370018), ('1722440', 0.0699186846613884), ('1720968', 0.09940698742866516)]\n"
     ]
    }
   ],
   "source": [
    "# top_1 = []\n",
    "# top_2 = []\n",
    "# top_3 = []\n",
    "# top_4 = []\n",
    "# top_5 = []\n",
    "\n",
    "# for i, row in df.iterrows():\n",
    "#     top_1.append(row[\"logits\"][0])\n",
    "#     top_2.append(row[\"logits\"][1])\n",
    "#     top_3.append(row[\"logits\"][2])\n",
    "#     top_4.append(row[\"logits\"][3])\n",
    "#     top_5.append(row[\"logits\"][4])\n",
    "\n",
    "#     print(i)\n",
    "#     # pprint(row)\n",
    "\n",
    "#     if i >= 5:\n",
    "#         break\n",
    "\n",
    "# print(f\"top_1: {top_1}\")\n",
    "# print(f\"top_2: {top_2}\")\n",
    "# print(f\"top_3: {top_3}\")\n",
    "# print(f\"top_4: {top_4}\")\n",
    "# print(f\"top_5: {top_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(top_species_ids): 30\n",
      "len(set(top_species_ids)): 25\n"
     ]
    }
   ],
   "source": [
    "# top_species_ids = [s_id for s_id, _ in [*top_1, *top_2, *top_3, *top_4, *top_5]]\n",
    "\n",
    "\n",
    "# print(f\"len(top_species_ids): {len(top_species_ids)}\")\n",
    "# print(f\"len(set(top_species_ids)): {len(set(top_species_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1654010', 0.44266772270202637)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row[\"logits\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_311770/1159756206.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.apply(select_top_k_unique_logits, top_k=top_k)\n"
     ]
    }
   ],
   "source": [
    "# dfg = df.groupby(\"image_name\")\n",
    "# pred_df = prepare_submission_csv(ds, top_k=top_k)\n",
    "# pred_df.shape\n",
    "\n",
    "# pred_df.groupby(\"image_name\").describe()\n",
    "# for k, v in pred_df.groupby(\"image_name\"):\n",
    "#     print(k)\n",
    "#     print(v)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare manual nested list of lists unpacking vs. more_itertools.flatten\n",
    "\n",
    "# logits = v[\"logits\"].to_list()\n",
    "# logits = [kth_logit for tile in logits for kth_logit in tile]\n",
    "# iter_logits = list(flatten(v[\"logits\"].to_list()))\n",
    "\n",
    "# for i in range(len(logits)):\n",
    "#     assert logits[i] == iter_logits[i], f\"Mismatch at index {i}: {logits[i]} != {iter_logits[i]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\n",
      "    \u001b[1m(\u001b[0m\u001b[32m'logits'\u001b[0m, \u001b[32m'logits_unique'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1654010'\u001b[0m, \u001b[1;36m0.44266772270202637\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1654010'\u001b[0m, \u001b[1;36m0.44266772270202637\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1395063'\u001b[0m, \u001b[1;36m0.3262457549571991\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1395063'\u001b[0m, \u001b[1;36m0.3262457549571991\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1392662'\u001b[0m, \u001b[1;36m0.3107529878616333\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1392662'\u001b[0m, \u001b[1;36m0.3107529878616333\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1392662'\u001b[0m, \u001b[1;36m0.293470174074173\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1414387'\u001b[0m, \u001b[1;36m0.2375417947769165\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1414387'\u001b[0m, \u001b[1;36m0.2375417947769165\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1743646'\u001b[0m, \u001b[1;36m0.17830075323581696\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1743646'\u001b[0m, \u001b[1;36m0.17830075323581696\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1395117'\u001b[0m, \u001b[1;36m0.13791653513908386\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1395117'\u001b[0m, \u001b[1;36m0.13791653513908386\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1393342'\u001b[0m, \u001b[1;36m0.09817499667406082\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1393342'\u001b[0m, \u001b[1;36m0.09817499667406082\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1664443'\u001b[0m, \u001b[1;36m0.09296201169490814\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1664443'\u001b[0m, \u001b[1;36m0.09296201169490814\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1743924'\u001b[0m, \u001b[1;36m0.08138307929039001\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1743924'\u001b[0m, \u001b[1;36m0.08138307929039001\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1738042'\u001b[0m, \u001b[1;36m0.08116285502910614\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1738042'\u001b[0m, \u001b[1;36m0.08116285502910614\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1390933'\u001b[0m, \u001b[1;36m0.07437077164649963\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1390933'\u001b[0m, \u001b[1;36m0.07437077164649963\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1390760'\u001b[0m, \u001b[1;36m0.07037860155105591\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1390760'\u001b[0m, \u001b[1;36m0.07037860155105591\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1558087'\u001b[0m, \u001b[1;36m0.06827252358198166\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1558087'\u001b[0m, \u001b[1;36m0.06827252358198166\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1392540'\u001b[0m, \u001b[1;36m0.06060715764760971\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1395117'\u001b[0m, \u001b[1;36m0.06622409075498581\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1741597'\u001b[0m, \u001b[1;36m0.05828603729605675\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1395117'\u001b[0m, \u001b[1;36m0.06265166401863098\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1699423'\u001b[0m, \u001b[1;36m0.05406714975833893\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1392540'\u001b[0m, \u001b[1;36m0.06060715764760971\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1394523'\u001b[0m, \u001b[1;36m0.05156639218330383\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1741597'\u001b[0m, \u001b[1;36m0.05828603729605675\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1550275'\u001b[0m, \u001b[1;36m0.04516402259469032\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1699423'\u001b[0m, \u001b[1;36m0.05406714975833893\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1395135'\u001b[0m, \u001b[1;36m0.03854033350944519\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'1394523'\u001b[0m, \u001b[1;36m0.05156639218330383\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1664563'\u001b[0m, \u001b[1;36m0.03685739263892174\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
      "\u001b[1m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from rich import print as pprint\n",
    "\n",
    "# out = [(\"logits\", \"logits_unique\")]\n",
    "# for i in range(20):\n",
    "#     out.append((logits[i], logits_unique[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2911"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_classification_dataframe(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    predictions: np.array,\n",
    "    similarities: np.array,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a classification DataFrame with Faiss predictions, similarities, and resolved species IDs.\n",
    "\n",
    "    :param train_df: Train DataFrame with image_name to species_id mapping\n",
    "    :param test_df: Test DataFrame (contains image_name, data, embeddings, etc.)\n",
    "    :param predictions: np.array of shape (N, K) with predicted image names\n",
    "    :param similarities: np.array of shape (N, K) with similarity scores\n",
    "    :return: DataFrame with columns: predictions, similarities, species_ids\n",
    "    \"\"\"\n",
    "    cls_test_df = test_df.copy()\n",
    "    cls_test_df[\"predictions\"] = predictions.tolist()\n",
    "    cls_test_df[\"similarities\"] = similarities.tolist()\n",
    "    # create lookup dictionary\n",
    "    image_to_species = dict(zip(train_df[\"image_name\"], train_df[\"species_id\"]))\n",
    "    # map preds to species_id\n",
    "    species_ids = []\n",
    "    for row in cls_test_df[\"predictions\"]:\n",
    "        row_species = [image_to_species.get(img_name, None) for img_name in row]\n",
    "        species_ids.append(row_species)\n",
    "    # add to DataFrame\n",
    "    cls_test_df[\"pred_species_ids\"] = species_ids\n",
    "    return cls_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1359714': 0.015930160880088806,\n",
       " '1392540': 0.06060715764760971,\n",
       " '1392662': 0.293470174074173,\n",
       " '1394523': 0.05156639218330383,\n",
       " '1628936': 0.025065019726753235}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len({k: v for k, v in ds[1][\"logits\"].items() if v is not None})\n",
    "# {k: v for k, v in ds[1][\"logits\"].items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=518, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(518, 518))\n",
       "    MaybeToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plantclef.pytorch.model import DINOv2LightningModel\n",
    "\n",
    "top_k = 5\n",
    "model = DINOv2LightningModel(top_k=top_k)\n",
    "model.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings and logits from model.predict_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9, 768])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# grid_size = cfg.grid_size\n",
    "\n",
    "# embeddings, logits = model.predict_step(\n",
    "#             batch, batch_idx=0\n",
    "#         )\n",
    "\n",
    "# embeddings, logits = model.predict_grid_step(\n",
    "#             batch, batch_idx=0\n",
    "#         )\n",
    "\n",
    "# print(embeddings.shape)\n",
    "# print(len(logits))\n",
    "# print(embeddings.shape)\n",
    "# embeddings = embeddings.view(-1, grid_size**2, 768)\n",
    "# print(embeddings.shape)\n",
    "# embeddings = embeddings.view(-1, grid_size**2, 768)\n",
    "\n",
    "# logits = [\n",
    "#             logits[i : i + grid_size**2] for i in range(0, len(logits), grid_size**2)\n",
    "#         ]\n",
    "# print(embeddings.shape)\n",
    "# print(len(logits))\n",
    "# print(embeddings.shape)\n",
    "# print(len(logits))\n",
    "# print([l.keys() for l in logits])\n",
    "# logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size -- len(logits): 36\n",
      "grid_size**2 -- len(logits[0]): 5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_size**2 -- len(logits[0]): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(logits[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# logits_img0_tile0 = logits[0][0]\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k -- k = len(list(logits[0][0].keys())): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# print(f\"batch_size -- len(logits): {len(logits)}\")\n",
    "# print(f\"grid_size**2 -- len(logits[0]): {len(logits[0])}\")\n",
    "# # logits_img0_tile0 = logits[0][0]\n",
    "# print(f\"top_k -- k = len(list(logits[0][0].keys())): {len(list(logits[0][0].keys()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image names from HFDataset -> Create a pandas DataFrame to match image names to logits + embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings, logits = model.predict_grid_step(\n",
    "#             batch, batch_idx=0\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_predictions_df(\n",
    "#     ds: HFPlantDataset, embeddings: torch.Tensor, logits: list\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Accepts an HFPlantDataset and a set of embeddings and logits.\n",
    "\n",
    "#     To be called after the model has been run on the full dataset in ds.\n",
    "\n",
    "#     Returns a DataFrame with the following columns:\n",
    "#         - image_name\n",
    "#         - tile\n",
    "#         - embeddings\n",
    "#         - logits\n",
    "#     The DataFrame is exploded to have one row per tile.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     pred_df = pd.DataFrame({\"image_name\": ds.dataset[\"file_path\"]})\n",
    "#     pred_df[\"image_name\"] = pred_df[\"image_name\"].str.rsplit(\"/\", n=1, expand=True)[1]\n",
    "\n",
    "#     pred_df = pred_df.convert_dtypes()\n",
    "\n",
    "#     pred_df = pred_df.assign(embeddings=embeddings.cpu().tolist(), logits=logits)\n",
    "#     explode_df = pred_df.explode([\"embeddings\", \"logits\"], ignore_index=True)\n",
    "#     explode_df = explode_df.assign(tile=explode_df.groupby(\"image_name\").cumcount())\n",
    "\n",
    "#     return explode_df\n",
    "\n",
    "\n",
    "# pred_ds = HFDataset.from_pandas(explode_df)\n",
    "# pred_ds.save_to_disk(test_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_ds = Dataset.load_from_disk(test_embeddings_path)\n",
    "# loaded_ds.features[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   file_path   4 non-null      string\n",
      " 1   embeddings  4 non-null      object\n",
      " 2   logits      4 non-null      object\n",
      "dtypes: object(2), string(1)\n",
      "memory usage: 224.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def write_embeddings_to_parquet(\n",
    "    df: pd.DataFrame,\n",
    "    folder_name: str,\n",
    "    num_partitions: int = 20,\n",
    "):\n",
    "    # path to data\n",
    "    root = Path().resolve().parents[0]\n",
    "    data_path = f\"{root}/data/embeddings\"\n",
    "    output_path = f\"{data_path}/{folder_name}\"\n",
    "\n",
    "    # remove existing data if it exists to avoid duplication\n",
    "    if Path(output_path).exists():\n",
    "        shutil.rmtree(output_path, ignore_errors=True)\n",
    "\n",
    "    # convert logits to json strings\n",
    "    df[\"logits\"] = df[\"logits\"].apply(json.dumps)\n",
    "\n",
    "    # assign partition numbers (0 to num_partitions-1)\n",
    "    df_size = len(df)\n",
    "    df[\"partition\"] = np.repeat(\n",
    "        np.arange(num_partitions), np.ceil(df_size / num_partitions)\n",
    "    )[:df_size]\n",
    "\n",
    "    # write to parquet using the new partition column\n",
    "    df.to_parquet(output_path, partition_cols=[\"partition\"], index=False)\n",
    "\n",
    "    print(\n",
    "        f\"Embedding dataset written to: {output_path} with {num_partitions} partitions.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# write data\n",
    "# folder_name = f\"test_grid_{GRID_SIZE}x{GRID_SIZE}_embeddings\"\n",
    "# write_embeddings_to_parquet(test_explode_df, folder_name, num_partitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.plot_image_tiles(idx=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.dataset = ds.dataset.take(100)\n",
    "# # extract embeddings\n",
    "# embeddings, logits = torch_pipeline(\n",
    "#     dataset=ds,  # .dataset.take(5),\n",
    "#     batch_size=2,\n",
    "#     use_grid=True,\n",
    "#     cpu_count=1,\n",
    "# )\n",
    "# embeddings.shape\n",
    "# grid_size = 3\n",
    "\n",
    "# embeddings = embeddings.view(-1, grid_size**2, 768)\n",
    "# embeddings.shape\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# img = ds._get_image_tensor(0)\n",
    "\n",
    "# plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(image: torch.Tensor) -> torch.Tensor:\n",
    "    min_dim = min(image.shape[1:])\n",
    "    return transforms.CenterCrop(min_dim)(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save huggingface test set to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting file paths in /teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images: 100%|██████████| 2105/2105 [00:00<00:00, 837905.47it/s]\n",
      "Walking through dir /teamspace/studios/this_studio/plantclef-vision/data/plantclef2025/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images: 1it [00:00, 20.22it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7550fa41ec4b4ebb40c9c2b7182dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/18 shards):   0%|          | 0/2105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image_list = collect_image_filepaths(dataset_dir)\n",
    "\n",
    "# ds = Dataset.from_dict({\"image\": image_list})\n",
    "# ds = ds.cast_column(\"image\", Image())\n",
    "\n",
    "# ds.save_to_disk(hf_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d29f25ba1748439d3c32c989d6d6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ds_loaded = Dataset.load_from_disk(hf_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transform(image_size: int) -> Callable:\n",
    "    \"\"\"Create image transformation pipeline that maintains aspect ratio.\"\"\"\n",
    "    transform_list = [\n",
    "        # transforms.ToPILImage(),\n",
    "        transforms.Resize(\n",
    "            image_size, max_size=image_size + 2\n",
    "        ),  # Maintains aspect ratio\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    "\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ds_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from plantclef.config import get_device\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Get list of stored filed in cloud bucket\n",
    "root = Path().resolve().parents[0]\n",
    "print(root)\n",
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "test_parquet_output_dir = \"/teamspace/studios/this_studio/plantclef-vision/data/parquet/plantclef2025/full_test\"\n",
    "os.makedirs(test_parquet_output_dir, exist_ok=True)\n",
    "\n",
    "root = \"/teamspace/studios/this_studio/plantclef-vision/data/plantclef2025\"\n",
    "test_image_dir = (\n",
    "    root + \"/competition-metadata/PlantCLEF2025_test_images/PlantCLEF2025_test_images\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting embeddings from single-label training images\n",
    "\n",
    "We extract embeddings from a small subset of training images to validate our pipeline.  \n",
    "We don't perform tiling on the train images (we use the full image) and extract 768-dimensional ViT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_train_df = pd.DataFrame({})\n",
    "\n",
    "# extract embeddings\n",
    "embeddings, logits = torch_pipeline(\n",
    "    limit_train_df,\n",
    "    batch_size=2,\n",
    "    use_grid=False,\n",
    "    cpu_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings shape\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first embedding\n",
    "embeddings[0][0][:100]  # showing first 100 values out of 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings dataframe\n",
    "cols = [\"image_name\", \"data\", \"species\", \"species_id\"]\n",
    "embeddings_df = limit_train_df[cols].copy()\n",
    "embeddings_df[\"embeddings\"] = embeddings.tolist()\n",
    "embeddings_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_images_from_binary\n",
    "\n",
    "embeddings_df = pd.DataFrame()\n",
    "plot_images_from_binary(\n",
    "    embeddings_df,\n",
    "    data_col=\"data\",\n",
    "    label_col=\"species\",\n",
    "    grid_size=(1, 2),\n",
    "    crop_square=True,\n",
    "    figsize=(8, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_embeddings\n",
    "\n",
    "plot_embeddings(\n",
    "    embeddings_df,\n",
    "    data_col=\"embeddings\",\n",
    "    label_col=\"species\",\n",
    "    grid_size=(1, 2),\n",
    "    figsize=(8, 4),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding test images with tiling (3x3)\n",
    "\n",
    "\n",
    "Since the test images are high-resolution and contain multiple plant species, we split them into a 3x3 grid of tiles.\n",
    "- We **extract embeddings** and **top-*K* logits** from each tile using the ViT model.  \n",
    "- This **patch-wise representation** is critical for enabling multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "USE_GRID = True\n",
    "GRID_SIZE = 3  # 3x3 grid of tiles\n",
    "CPU_COUNT = 1  # custom cpu_count\n",
    "TOP_K = 5  # top-K logits for each tile\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame({})\n",
    "test_image_df = pd.DataFrame({})\n",
    "\n",
    "# select images from test set\n",
    "image_names = [\"CBN-Pyr-03-20230706.jpg\", \"CBN-can-E6-20230706.jpg\"]\n",
    "test_image_df = test_df[test_df[\"image_name\"].isin(image_names)]\n",
    "\n",
    "# get embeddings and logits\n",
    "embeddings, logits = torch_pipeline(\n",
    "    test_image_df,\n",
    "    batch_size=2,\n",
    "    use_grid=USE_GRID,\n",
    "    grid_size=GRID_SIZE,\n",
    "    cpu_count=CPU_COUNT,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings shape\n",
    "embeddings.shape  # (2, 9, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings dataframe\n",
    "def explode_embeddings_logits(\n",
    "    df: pd.DataFrame,\n",
    "    embeddings: torch.Tensor,\n",
    "    logits: list,\n",
    "    cols: list = [\"image_name\", \"data\"],\n",
    ") -> pd.DataFrame:\n",
    "    # create dataframe\n",
    "    pred_df = df[cols].copy()\n",
    "    pred_df[\"embeddings\"] = embeddings.cpu().tolist()\n",
    "    pred_df[\"logits\"] = logits\n",
    "    # explode embeddings\n",
    "    explode_df = pred_df.explode([\"embeddings\", \"logits\"], ignore_index=True)\n",
    "    # assign tile number for each image\n",
    "    explode_df[\"tile\"] = explode_df.groupby(\"image_name\").cumcount()\n",
    "    return explode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_df = explode_embeddings_logits(test_image_df, embeddings, logits)\n",
    "explode_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_image_tiles\n",
    "\n",
    "# show image tiles\n",
    "plot_image_tiles(\n",
    "    explode_df,\n",
    "    data_col=\"data\",\n",
    "    grid_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_embed_tiles\n",
    "\n",
    "plot_embed_tiles(\n",
    "    explode_df,\n",
    "    data_col=\"embeddings\",\n",
    "    grid_size=3,\n",
    "    figsize=(15, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot grid embeddings\n",
    "plot_embeddings(\n",
    "    explode_df,\n",
    "    data_col=\"embeddings\",\n",
    "    label_col=\"tile\",\n",
    "    grid_size=(3, 3),\n",
    "    figsize=(8, 8),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing classifier logits per tile\n",
    "\n",
    "For each tile, we look at the **top predicted species** and associated confidence scores (`logits`).  \n",
    "This helps interpret how confident the model is in identifying species in each patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length logits: {len(logits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display logits of first tile\n",
    "explode_df[\"logits\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display logits for each tile\n",
    "for i in range(9):\n",
    "    logits = explode_df[\"logits\"].iloc[i]\n",
    "    logits_formatted = {k: round(v, 3) for k, v in logits.items()}\n",
    "    print(f\"Tile {i+1}: {logits_formatted}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the entire test set with tiling\n",
    "\n",
    "We scale up our embedding pipeline to process the full test dataset using **3x3 tiling**.  \n",
    "This prepares the data for the downstream tasks of efficient **nearest neighbor search** and **multi-label prediction** at the tile level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cpu_count = os.cpu_count()\n",
    "print(f\"CPU count: {cpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "USE_GRID = True\n",
    "GRID_SIZE = 3  # 3x3 grid of tiles\n",
    "CPU_COUNT = 1  # custom cpu_count\n",
    "TOP_K = 5  # top-K logits for each tile\n",
    "\n",
    "# get embeddings and logits\n",
    "test_embeddings, test_logits = torch_pipeline(\n",
    "    test_df,\n",
    "    batch_size=10,  # 10 imamges per batch\n",
    "    use_grid=USE_GRID,\n",
    "    grid_size=GRID_SIZE,\n",
    "    cpu_count=CPU_COUNT,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_embeddings.shape)\n",
    "print(len(test_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode full embeddings and logits\n",
    "test_explode_df = explode_embeddings_logits(\n",
    "    test_df,\n",
    "    test_embeddings,\n",
    "    test_logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_explode_df.shape)\n",
    "test_explode_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embed_tiles(\n",
    "    test_explode_df,\n",
    "    data_col=\"embeddings\",\n",
    "    grid_size=3,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving test embeddings and logits to Parquet\n",
    "\n",
    "We serialize the full test embeddings into partitioned Parquet files for later use in inference pipelines.  \n",
    "The logits are stored as JSON strings for flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_embeddings_to_parquet(\n",
    "#     df: pd.DataFrame,\n",
    "#     folder_name: str,\n",
    "#     num_partitions: int = 20,\n",
    "# ):\n",
    "#     # path to data\n",
    "#     root = Path().resolve().parents[0]\n",
    "#     data_path = f\"{root}/data/embeddings\"\n",
    "#     output_path = f\"{data_path}/{folder_name}\"\n",
    "\n",
    "#     # remove existing data if it exists to avoid duplication\n",
    "#     if Path(output_path).exists():\n",
    "#         shutil.rmtree(output_path, ignore_errors=True)\n",
    "\n",
    "#     # convert logits to json strings\n",
    "#     df[\"logits\"] = df[\"logits\"].apply(json.dumps)\n",
    "\n",
    "#     # assign partition numbers (0 to num_partitions-1)\n",
    "#     df_size = len(df)\n",
    "#     df[\"partition\"] = np.repeat(\n",
    "#         np.arange(num_partitions), np.ceil(df_size / num_partitions)\n",
    "#     )[:df_size]\n",
    "\n",
    "#     # write to parquet using the new partition column\n",
    "#     df.to_parquet(output_path, partition_cols=[\"partition\"], index=False)\n",
    "\n",
    "#     print(\n",
    "#         f\"Embedding dataset written to: {output_path} with {num_partitions} partitions.\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # write data\n",
    "# folder_name = f\"test_grid_{GRID_SIZE}x{GRID_SIZE}_embeddings\"\n",
    "# write_embeddings_to_parquet(test_explode_df, folder_name, num_partitions=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the full training set (no tiling)\n",
    "\n",
    "We repeat the embedding process on the **full training dataset**, this time *without tiling*.  \n",
    "This enables us to use the embeddings directly or as a **transfer learning** approach in a Faiss-based nearest neighbor retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "USE_GRID = False\n",
    "CPU_COUNT = 1  # custom cpu_count\n",
    "TOP_K = 5  # top-K logits for each tile\n",
    "\n",
    "train_df = pd.DataFrame({})\n",
    "\n",
    "# get embeddings and logits\n",
    "train_embeddings, train_logits = torch_pipeline(\n",
    "    train_df,\n",
    "    batch_size=20,  # 20 imamges per batch\n",
    "    use_grid=USE_GRID,\n",
    "    cpu_count=CPU_COUNT,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embeddings.shape)\n",
    "print(len(train_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode full embeddings and logits\n",
    "train_explode_df = explode_embeddings_logits(\n",
    "    train_df,\n",
    "    train_embeddings,\n",
    "    train_logits,\n",
    "    cols=[\"image_name\", \"data\", \"species\", \"species_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explode_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.plotting import plot_single_image_embeddings\n",
    "\n",
    "plot_single_image_embeddings(\n",
    "    train_explode_df,\n",
    "    num_images=2,\n",
    "    figsize=(8, 10),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the training embeddings to Parquet\n",
    "\n",
    "Finally, we save the full training embeddings in partitioned Parquet format to support fast, distributed retrieval during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data\n",
    "folder_name = \"train_embeddings\"\n",
    "write_embeddings_to_parquet(train_explode_df, folder_name, num_partitions=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Ready for Downstream Use\n",
    "\n",
    "We now have rich ViT embeddings for both train and test datasets, ready for use in:\n",
    "- Multi-label classification\n",
    "- Retrieval-based inference\n",
    "- Nearest Neighbor Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "data_path = f\"{root}/data/embeddings\"\n",
    "# output_path = f\"{data_path}/test_grid_3x3_embeddings\"\n",
    "output_path = f\"{data_path}/train_embeddings\"\n",
    "\n",
    "train_emb_df = pd.read_parquet(output_path)\n",
    "print(train_emb_df.shape)\n",
    "train_emb_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{data_path}/test_grid_3x3_embeddings\"\n",
    "test_grid_df = pd.read_parquet(output_path)\n",
    "print(test_grid_df.shape)\n",
    "test_grid_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
